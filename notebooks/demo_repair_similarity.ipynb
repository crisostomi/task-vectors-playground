{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import hydra\n",
    "import omegaconf\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch import Callback\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import enforce_tags, seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "from nn_core.serialization import NNCheckpointIO\n",
    "\n",
    "# Force the execution of __init__.py if this file is executed directly.\n",
    "import tvp  # noqa\n",
    "from tvp.data.datamodule import MetaData\n",
    "from tvp.data.datasets.registry import get_dataset\n",
    "from tvp.task_vectors.task_vectors import TaskVector\n",
    "from tvp.utils.io_utils import load_model_from_artifact\n",
    "from tvp.utils.utils import build_callbacks\n",
    "from torch.nn.utils import vector_to_parameters\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "pylogger = logging.getLogger(__name__)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"playground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"task_vectors\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index_everything(cfg)\n",
    "\n",
    "cfg.core.tags = enforce_tags(cfg.core.get(\"tags\", None))\n",
    "\n",
    "template_core: NNTemplateCore = NNTemplateCore(\n",
    "    restore_cfg=cfg.train.get(\"restore\", None),\n",
    ")\n",
    "logger: NNLogger = NNLogger(logging_cfg=cfg.train.logging, cfg=cfg, resume_id=template_core.resume_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "finetuned_models = {\n",
    "    dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "    for dataset in cfg.task_vectors.to_apply\n",
    "}\n",
    "\n",
    "zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResetConv(nn.Module):\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        self.out_channels = conv.out_channels\n",
    "        self.conv = conv\n",
    "        self.bn = nn.BatchNorm2d(self.out_channels)\n",
    "        self.rescale = False\n",
    "\n",
    "    def set_stats(self, goal_mean, goal_var, eps=1e-5):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        goal_std = (goal_var + eps).sqrt()\n",
    "        self.bn.weight.data = goal_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.rescale:\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResetLinear(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        # print(f\"\\n\\n[ResetLinear] layer: {layer}\\n\\n\")\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm1d(layer.out_features)\n",
    "        self.weight = layer.weight\n",
    "        self.bias = layer.bias\n",
    "\n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: [L, N, C] (in torch.nn.BatchNorm1d doc notation)\n",
    "        x = self.layer(x)\n",
    "\n",
    "        # match current shape to shape required by BatchNorm1d\n",
    "        x = x.permute(1, 2, 0)\n",
    "        # x.shape: [N, C, L] (in torch.nn.BatchNorm1d doc notation)\n",
    "\n",
    "        x = self.bn(x)\n",
    "\n",
    "        # match current shape to shape required by Linear\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # x.shape: [L, N, C] (in torch.nn.BatchNorm1d doc notation)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResetLayerNorm(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm1d(layer.normalized_shape[0])\n",
    "\n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return self.bn(x)\n",
    "\n",
    "\n",
    "def replace_layers(module):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Conv2d):\n",
    "            setattr(module, name, ResetConv(child))\n",
    "        # elif isinstance(child, nn.LayerNorm):\n",
    "        #     setattr(module, name, ResetLayerNorm(child))\n",
    "        elif isinstance(child, nn.Linear):\n",
    "            setattr(module, name, ResetLinear(child))\n",
    "        else:\n",
    "            replace_layers(child)\n",
    "\n",
    "\n",
    "def make_tracked_net(model):\n",
    "    tracked_model = copy.deepcopy(model)\n",
    "    replace_layers(tracked_model.model.visual)\n",
    "    return tracked_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap and compute stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\n",
    "    cfg.nn.data.train_dataset,\n",
    "    preprocess_fn=zeroshot_model.train_preprocess,\n",
    "    location=cfg.nn.data.data_path,\n",
    "    batch_size=cfg.nn.data.batch_size.train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def reset_bn_stats(model, epochs, loader):\n",
    "    \"\"\"\n",
    "    Reset batchnorm stats. We use the train loader with data augmentation as this gives better results.\n",
    "    \"\"\"\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None  # use simple average\n",
    "            m.reset_running_stats()\n",
    "\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader):\n",
    "                if isinstance(batch, Dict):\n",
    "                    input = batch[\"x\"]\n",
    "                else:\n",
    "                    input = batch[0]\n",
    "                    # print(f\"[reset_bn_stats] input.shape before model(): {input.shape}\")\n",
    "                out = model(input.cuda())\n",
    "                # print(f\"[reset_bn_stats] out.shape after model()   : {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'CIFAR100'\n",
    "dataset_name = cfg.nn.data.train_dataset.replace(\"Val\", \"\")\n",
    "finetuned_model = finetuned_models[dataset_name]\n",
    "tracked_finetuned_model = make_tracked_net(finetuned_models[dataset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_bn_stats(tracked_finetuned_model.cuda(), 1, dataset.train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_finetuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda model: parameters_to_vector(model.parameters())\n",
    "\n",
    "zeroshot_vec = flatten(zeroshot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors = [\n",
    "    TaskVector.from_models(zeroshot_model, finetuned_models[dataset]) for dataset in cfg.task_vectors.to_apply\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_task_vector(model, task_vector):\n",
    "    model.load_state_dict({k: v + task_vector[k] for k, v in model.state_dict().items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    task_vectors = torch.stack(\n",
    "        [flatten(finetuned_models[dataset]) - zeroshot_vec for dataset in cfg.task_vectors.to_apply]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors_sum = torch.sum(task_vectors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "multi_task_vector = task_vectors_sum / len(task_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_model = copy.deepcopy(zeroshot_model)\n",
    "vector_to_parameters(multi_task_vector, delta_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_equipped_model = copy.deepcopy(zeroshot_model)\n",
    "apply_task_vector(task_equipped_model, delta_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_identifier = f\"{cfg.nn.module.model.model_name}_{cfg.nn.data.dataset.dataset_name}_head\"\n",
    "classification_head = load_model_from_artifact(\n",
    "    artifact_path=f\"{classification_head_identifier}:latest\", run=logger.experiment\n",
    ")\n",
    "\n",
    "model = hydra.utils.instantiate(\n",
    "    cfg.nn.module, encoder=task_equipped_model, classifier=classification_head, _recursive_=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index_everything(cfg)\n",
    "\n",
    "dataset = get_dataset(\n",
    "    cfg.nn.data.train_dataset,\n",
    "    preprocess_fn=model.encoder.train_preprocess,\n",
    "    location=cfg.nn.data.data_path,\n",
    "    batch_size=cfg.nn.data.batch_size.train,\n",
    ")\n",
    "\n",
    "callbacks: List[Callback] = build_callbacks(cfg.train.callbacks, template_core)\n",
    "\n",
    "storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "pylogger.info(\"Instantiating the <Trainer>\")\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=storage_dir,\n",
    "    plugins=[NNCheckpointIO(jailing_dir=logger.run_dir)],\n",
    "    logger=False,\n",
    "    callbacks=callbacks,\n",
    "    **cfg.train.trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylogger.info(\"Evaluating on the training set\")\n",
    "# trainer.test(model=model, dataloaders=dataset.train_loader)\n",
    "\n",
    "pylogger.info(\"Evaluating on the test set!\")\n",
    "trainer.test(model=model, dataloaders=dataset.test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep this accuracy in mind, as it will be used as baseline to compare all upcoming experiments against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset stats\n",
    "\n",
    "Compute and export mean and std for every dataset, if not already present on disk.<br>\n",
    "Otherwhise, load mean and std for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the full path of a module\n",
    "def get_module_path(module, parent_name=\"\"):\n",
    "    if parent_name:\n",
    "        return f\"{parent_name}.{module._get_name()}\"\n",
    "    return module._get_name()\n",
    "\n",
    "\n",
    "# Define a recursive function to collect batch norm stats from 'Reset' layers\n",
    "def collect_bn_stats(model, parent_name=\"\"):\n",
    "    bn_stats = {}\n",
    "    for name, module in model.named_children():\n",
    "        # Construct the full path of the current module\n",
    "        module_path = f\"{parent_name}.{name}\" if parent_name else name\n",
    "\n",
    "        # Check for batch normalization layers within Reset layers\n",
    "        if isinstance(module, ResetConv) or isinstance(module, ResetLinear):\n",
    "            for sub_name, sub_module in module.named_children():\n",
    "                if isinstance(sub_module, nn.BatchNorm2d) or isinstance(sub_module, nn.BatchNorm1d):\n",
    "                    bn_stats[f\"{module_path}.{sub_name}\"] = {\n",
    "                        \"running_mean\": sub_module.running_mean.clone().detach(),\n",
    "                        \"running_var\": sub_module.running_var.clone().detach(),\n",
    "                        \"num_batches_tracked\": sub_module.num_batches_tracked.clone().detach(),\n",
    "                    }\n",
    "        # Recursively check submodules\n",
    "        bn_stats.update(collect_bn_stats(module, module_path))\n",
    "\n",
    "    return bn_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stats = {}  # S in board pictures\n",
    "\n",
    "# if \"./dataset_stats.pth\" is on disk, do not compute stats!\n",
    "populate_stats = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if populate_stats:\n",
    "    # TODO once done, try without it, as all methods use copy.deepcopy, so hygiene should be guaranteed\n",
    "\n",
    "    import copy\n",
    "\n",
    "    zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "    zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "    finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "    finetuned_models = {\n",
    "        dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "        for dataset in cfg.task_vectors.to_apply\n",
    "    }\n",
    "\n",
    "    zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if populate_stats:\n",
    "    for dataset_name in cfg.task_vectors.to_apply:\n",
    "        print(f\"[dataset_name]: {dataset_name}\\n\")\n",
    "\n",
    "        # begin load the dataset\n",
    "\n",
    "        dataset = get_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            preprocess_fn=zeroshot_model.train_preprocess,\n",
    "            location=cfg.nn.data.data_path,\n",
    "            batch_size=cfg.nn.data.batch_size.train,\n",
    "        )\n",
    "\n",
    "        #   end load the dataset\n",
    "\n",
    "        finetuned_model = copy.deepcopy(finetuned_models[dataset_name])\n",
    "\n",
    "        # begin compute layer-wise statistics\n",
    "\n",
    "        tracked_finetuned_model = make_tracked_net(finetuned_model)\n",
    "\n",
    "        reset_bn_stats(tracked_finetuned_model.cuda(), 1, dataset.train_loader)\n",
    "\n",
    "        # end compute layer-wise statistics\n",
    "\n",
    "        # begin store layer-wise statistics\n",
    "\n",
    "        dataset_stats[dataset_name] = collect_bn_stats(tracked_finetuned_model.model.visual)\n",
    "\n",
    "        # end store layer-wise statistics\n",
    "\n",
    "    torch.save(dataset_stats, \"./dataset_stats.pth\")\n",
    "\n",
    "else:\n",
    "    print(f\"Loading dataset stats from './dataset_stats.pth'...\")\n",
    "\n",
    "    dataset_stats = torch.load(\"./dataset_stats.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_anchor_batches = 1\n",
    "\n",
    "anchors = {}\n",
    "\n",
    "populate_anchors = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if populate_anchors:\n",
    "    # TODO once done, try without it, as all methods use copy.deepcopy, so hygene should be guaranteed\n",
    "\n",
    "    import copy\n",
    "\n",
    "    zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "    zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "    finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "    finetuned_models = {\n",
    "        dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "        for dataset in cfg.task_vectors.to_apply\n",
    "    }\n",
    "\n",
    "    zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if populate_anchors:\n",
    "    for dataset_name in cfg.task_vectors.to_apply:\n",
    "        print(f\"[dataset_name]: {dataset_name}\\n\")\n",
    "\n",
    "        dataset = get_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            preprocess_fn=zeroshot_model.train_preprocess,\n",
    "            location=cfg.nn.data.data_path,\n",
    "            batch_size=cfg.nn.data.batch_size.train,\n",
    "        )\n",
    "\n",
    "        model: nn.Module = copy.deepcopy(finetuned_models[dataset_name])\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for id, train_batch in enumerate(dataset.train_loader):\n",
    "                if id == n_anchor_batches:\n",
    "                    break\n",
    "\n",
    "                if isinstance(train_batch, Dict):\n",
    "                    input: torch.Tensor = train_batch[\"x\"]\n",
    "                else:\n",
    "                    input: torch.Tensor = train_batch[0]\n",
    "\n",
    "                anchors[dataset_name] = model.cuda()(input.cuda())\n",
    "\n",
    "    torch.save(anchors, \"./anchors.pth\")\n",
    "\n",
    "else:\n",
    "    print(f\"Loading anchors from './anchors.pth'...\")\n",
    "\n",
    "    anchors = torch.load(\"./anchors.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_tensor = torch.stack([anchors[dataset_name] for dataset_name in anchors.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate((anchors.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO once done, try without it, as all methods use copy.deepcopy, so hygene should be guaranteed\n",
    "\n",
    "import copy\n",
    "\n",
    "zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "finetuned_models = {\n",
    "    dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "    for dataset in cfg.task_vectors.to_apply\n",
    "}\n",
    "\n",
    "zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"GTSRB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_index_everything(cfg)\n",
    "\n",
    "dataset = get_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    preprocess_fn=zeroshot_model.train_preprocess,\n",
    "    location=cfg.nn.data.data_path,\n",
    "    batch_size=cfg.nn.data.batch_size.train,\n",
    ")\n",
    "\n",
    "callbacks: List[Callback] = build_callbacks(cfg.train.callbacks, template_core)\n",
    "\n",
    "storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "pylogger.info(\"Instantiating the <Trainer>\")\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=storage_dir,\n",
    "    plugins=[NNCheckpointIO(jailing_dir=logger.run_dir)],\n",
    "    logger=False,\n",
    "    callbacks=callbacks,\n",
    "    **cfg.train.trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply all task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda model: parameters_to_vector(model.parameters())\n",
    "\n",
    "zeroshot_vec = flatten(zeroshot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors = [\n",
    "    TaskVector.from_models(zeroshot_model, finetuned_models[dataset]) for dataset in cfg.task_vectors.to_apply\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    task_vectors = torch.stack(\n",
    "        [flatten(finetuned_models[dataset]) - zeroshot_vec for dataset in cfg.task_vectors.to_apply]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors_sum = torch.sum(task_vectors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "multi_task_vector = task_vectors_sum / len(task_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_model = copy.deepcopy(zeroshot_model)\n",
    "vector_to_parameters(multi_task_vector, delta_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_equipped_model = copy.deepcopy(zeroshot_model)\n",
    "apply_task_vector(task_equipped_model, delta_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_identifier = f\"{cfg.nn.module.model.model_name}_{dataset_name}_head\"\n",
    "classification_head = load_model_from_artifact(\n",
    "    artifact_path=f\"{classification_head_identifier}:latest\", run=logger.experiment\n",
    ")\n",
    "\n",
    "model = hydra.utils.instantiate(\n",
    "    cfg.nn.module, encoder=task_equipped_model, classifier=classification_head, _recursive_=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate (naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(\"Evaluating on the test set!\")\n",
    "trainer.test(model=model, dataloaders=dataset.test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (our method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO once done, try without it, as all methods use copy.deepcopy, so hygene should be guaranteed\n",
    "\n",
    "import copy\n",
    "\n",
    "zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "finetuned_models = {\n",
    "    dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "    for dataset in cfg.task_vectors.to_apply\n",
    "}\n",
    "\n",
    "zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_index_everything(cfg)\n",
    "\n",
    "dataset = get_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    preprocess_fn=zeroshot_model.train_preprocess,\n",
    "    location=cfg.nn.data.data_path,\n",
    "    batch_size=cfg.nn.data.batch_size.train,\n",
    ")\n",
    "\n",
    "callbacks: List[Callback] = build_callbacks(cfg.train.callbacks, template_core)\n",
    "\n",
    "storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "pylogger.info(\"Instantiating the <Trainer>\")\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=storage_dir,\n",
    "    plugins=[NNCheckpointIO(jailing_dir=logger.run_dir)],\n",
    "    logger=False,\n",
    "    callbacks=callbacks,\n",
    "    **cfg.train.trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply all task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda model: parameters_to_vector(model.parameters())\n",
    "\n",
    "zeroshot_vec = flatten(zeroshot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors = [\n",
    "    TaskVector.from_models(zeroshot_model, finetuned_models[dataset]) for dataset in cfg.task_vectors.to_apply\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    task_vectors = torch.stack(\n",
    "        [flatten(finetuned_models[dataset]) - zeroshot_vec for dataset in cfg.task_vectors.to_apply]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors_sum = torch.sum(task_vectors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "multi_task_vector = task_vectors_sum / len(task_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_model = copy.deepcopy(zeroshot_model)\n",
    "vector_to_parameters(multi_task_vector, delta_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_equipped_model = copy.deepcopy(zeroshot_model)\n",
    "apply_task_vector(task_equipped_model, delta_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model for REPAIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep this here, as the classification head may have layers that may be detected as REPAIRable, even tho we don't want to repair them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_finetuned_model = make_tracked_net(task_equipped_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate (our method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_identifier = f\"{cfg.nn.module.model.model_name}_{dataset_name}_head\"\n",
    "classification_head = load_model_from_artifact(\n",
    "    artifact_path=f\"{classification_head_identifier}:latest\", run=logger.experiment\n",
    ")\n",
    "\n",
    "model = hydra.utils.instantiate(\n",
    "    cfg.nn.module, encoder=tracked_finetuned_model, classifier=classification_head, _recursive_=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(similarity: torch.Tensor):\n",
    "    # similarity.shape: [B, M]\n",
    "\n",
    "    # Get the index of the highest similarity for each element in the batch\n",
    "    max_similarity_indices = torch.argmax(similarity, dim=1)  # shape: [B]\n",
    "\n",
    "    # Perform majority voting\n",
    "    majority_vote = Counter(max_similarity_indices.cpu().numpy()).most_common(1)[0][0]\n",
    "\n",
    "    return majority_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_based_similarity(batch: torch.Tensor, anchors_tensor: torch.Tensor):\n",
    "    # batch.shape: [B, D]\n",
    "    # anchors_tensor.shape: [M, K, D]\n",
    "\n",
    "    M, K, D = anchors_tensor.shape\n",
    "    B = batch.shape[0]\n",
    "\n",
    "    # Compute centroids of each anchor set\n",
    "    centroids = anchors_tensor.mean(dim=1)  # shape: [M, D]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    # Normalize batch and centroids\n",
    "    batch_norm = F.normalize(batch, p=2, dim=1)  # shape: [B, D]\n",
    "    centroids_norm = F.normalize(centroids, p=2, dim=1)  # shape: [M, D]\n",
    "\n",
    "    # Compute similarity\n",
    "    similarity = torch.mm(batch_norm, centroids_norm.t())  # shape: [B, M]\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_wise_majority_votes = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Iterate through the test data loader and compute point-to-set distances\n",
    "for id, test_batch in tqdm(list(enumerate(dataset.test_loader))):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(test_batch, dict):\n",
    "            input: torch.Tensor = test_batch[\"x\"]\n",
    "        else:\n",
    "            input: torch.Tensor = test_batch[0]\n",
    "\n",
    "        emb = finetuned_models[dataset_name].cuda()(input.cuda())  # [B, D]\n",
    "        # print(f\"[emb]: {emb.shape}\")\n",
    "\n",
    "        similarities = centroid_based_similarity(emb, anchors_tensor)\n",
    "        # print(f\"[similarities]: {similarities.shape}\")\n",
    "\n",
    "        batch_wise_majority_votes.append(majority_voting(similarities))\n",
    "        # print(f\"[majority_vote]: {batch_wise_majority_votes[-1]}\")\n",
    "\n",
    "batch_wise_majority_votes = np.asarray(batch_wise_majority_votes)\n",
    "most_similar_dataset_id = Counter(batch_wise_majority_votes).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_dataset = list(anchors.keys())[most_similar_dataset_id]\n",
    "most_similar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_dataset = get_dataset(\n",
    "    dataset_name=most_similar_dataset,\n",
    "    preprocess_fn=model.encoder.train_preprocess,\n",
    "    location=cfg.nn.data.data_path,\n",
    "    batch_size=cfg.nn.data.batch_size.train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.model.visual.conv1.bn.running_mean[:10]  # should be all zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_bn_stats(model.cuda(), 1, most_similar_dataset.train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.model.visual.conv1.bn.running_mean[:10]  # should NOT be all zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(\"Evaluating on the test set (stats set via reset_bn_stats()!)\")\n",
    "trainer.test(model=model, dataloaders=dataset.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.model.visual.conv1.bn.running_mean = dataset_stats[dataset_name][\"conv1.bn\"][\"running_mean\"]\n",
    "model.encoder.model.visual.conv1.bn.running_var = dataset_stats[dataset_name][\"conv1.bn\"][\"running_var\"]\n",
    "\n",
    "for i in range(12):\n",
    "    model.encoder.model.visual.transformer.resblocks[i].attn.out_proj.bn.running_mean = dataset_stats[dataset_name][\n",
    "        f\"transformer.resblocks.{i}.attn.out_proj.bn\"\n",
    "    ][\"running_mean\"]\n",
    "    model.encoder.model.visual.transformer.resblocks[i].mlp.c_fc.bn.running_mean = dataset_stats[dataset_name][\n",
    "        f\"transformer.resblocks.{i}.mlp.c_fc.bn\"\n",
    "    ][\"running_mean\"]\n",
    "    model.encoder.model.visual.transformer.resblocks[i].mlp.c_proj.bn.running_mean = dataset_stats[dataset_name][\n",
    "        f\"transformer.resblocks.{i}.mlp.c_proj.bn\"\n",
    "    ][\"running_mean\"]\n",
    "\n",
    "    model.encoder.model.visual.transformer.resblocks[i].attn.out_proj.bn.running_var = dataset_stats[dataset_name][\n",
    "        f\"transformer.resblocks.{i}.attn.out_proj.bn\"\n",
    "    ][\"running_var\"]\n",
    "    model.encoder.model.visual.transformer.resblocks[i].mlp.c_fc.bn.running_var = dataset_stats[dataset_name][\n",
    "        f\"transformer.resblocks.{i}.mlp.c_fc.bn\"\n",
    "    ][\"running_var\"]\n",
    "    model.encoder.model.visual.transformer.resblocks[i].mlp.c_proj.bn.running_var = dataset_stats[dataset_name][\n",
    "        f\"transformer.resblocks.{i}.mlp.c_proj.bn\"\n",
    "    ][\"running_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(\"Evaluating on the test set! (stats set via dataset_stats)\")\n",
    "trainer.test(model=model, dataloaders=dataset.test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
