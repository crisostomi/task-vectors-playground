{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/data/datamodule.py:193: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=str(PROJECT_ROOT / \"conf\"), config_name=\"default\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import hydra\n",
    "import omegaconf\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from lightning.pytorch import Callback\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import enforce_tags, seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "from nn_core.serialization import NNCheckpointIO\n",
    "\n",
    "# Force the execution of __init__.py if this file is executed directly.\n",
    "import tvp  # noqa\n",
    "from tvp.data.datamodule import MetaData\n",
    "from tvp.data.datasets.registry import get_dataset\n",
    "from tvp.task_vectors.task_vectors import TaskVector\n",
    "from tvp.utils.io_utils import load_model_from_artifact\n",
    "from tvp.utils.utils import build_callbacks\n",
    "from torch.nn.utils import vector_to_parameters\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "pylogger = logging.getLogger(__name__)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"playground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"task_vectors\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1608637542\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:01 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Setting seed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1608637542</span> from seeds<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>                         <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nn_core.common.utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py#107\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:01\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Setting seed \u001b[1;36m1608637542\u001b[0m from seeds\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m                         \u001b]8;id=506654;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py\u001b\\\u001b[2mnn_core.common.utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=125355;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py#107\u001b\\\u001b[2m107\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Tags: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'dev'</span><span style=\"font-weight: bold\">]</span>                                                  <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nn_core.common.utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py#96\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">96</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Tags: \u001b[1m[\u001b[0m\u001b[32m'dev'\u001b[0m\u001b[1m]\u001b[0m                                                  \u001b]8;id=507214;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py\u001b\\\u001b[2mnn_core.common.utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=552910;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/common/utils.py#96\u001b\\\u001b[2m96\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Restoring with mode: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold; font-style: italic\">None</span><span style=\"font-weight: bold\">&gt;</span>                                         <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/resume.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nn_core.resume</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/resume.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Restoring with mode: \u001b[1m<\u001b[0m\u001b[1;3;35mNone\u001b[0m\u001b[1m>\u001b[0m                                         \u001b]8;id=346224;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/resume.py\u001b\\\u001b[2mnn_core.resume\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=130475;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/resume.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Instantiating <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">WandbLogger</span><span style=\"font-weight: bold\">&gt;</span>                                   <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/model_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nn_core.model_logging</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/model_logging.py#41\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Instantiating \u001b[1m<\u001b[0m\u001b[1;95mWandbLogger\u001b[0m\u001b[1m>\u001b[0m                                   \u001b]8;id=170108;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/model_logging.py\u001b\\\u001b[2mnn_core.model_logging\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=507338;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/nn_core/model_logging.py#41\u001b\\\u001b[2m41\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:02 </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Failed to detect the name of this notebook, you can set it manually  <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/wandb/jupyter.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">wandb.jupyter</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/wandb/jupyter.py#224\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">224</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         with the WANDB_NOTEBOOK_NAME environment variable to enable code     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         saving.                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:02\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Failed to detect the name of this notebook, you can set it manually  \u001b]8;id=25814;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/wandb/jupyter.py\u001b\\\u001b[2mwandb.jupyter\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=720665;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/wandb/jupyter.py#224\u001b\\\u001b[2m224\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         with the WANDB_NOTEBOOK_NAME environment variable to enable code     \u001b[2m                 \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         saving.                                                              \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdansolombrino\u001b[0m (\u001b[33mgladia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240529_200403-dr80wvxr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gladia/task-vectors-playground/runs/dr80wvxr' target=\"_blank\">denim-sunset-183</a></strong> to <a href='https://wandb.ai/gladia/task-vectors-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gladia/task-vectors-playground' target=\"_blank\">https://wandb.ai/gladia/task-vectors-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gladia/task-vectors-playground/runs/dr80wvxr' target=\"_blank\">https://wandb.ai/gladia/task-vectors-playground/runs/dr80wvxr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_index_everything(cfg)\n",
    "\n",
    "cfg.core.tags = enforce_tags(cfg.core.get(\"tags\", None))\n",
    "\n",
    "template_core: NNTemplateCore = NNTemplateCore(\n",
    "    restore_cfg=cfg.train.get(\"restore\", None),\n",
    ")\n",
    "logger: NNLogger = NNLogger(logging_cfg=cfg.train.logging, cfg=cfg, resume_id=template_core.resume_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "finetuned_models = {\n",
    "    dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "    for dataset in cfg.task_vectors.to_apply\n",
    "}\n",
    "\n",
    "zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResetConv(nn.Module):\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        self.out_channels = conv.out_channels\n",
    "        self.conv = conv\n",
    "        self.bn = nn.BatchNorm2d(self.out_channels)\n",
    "        self.rescale = False\n",
    "\n",
    "    def set_stats(self, goal_mean, goal_var, eps=1e-5):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        goal_std = (goal_var + eps).sqrt()\n",
    "        self.bn.weight.data = goal_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.rescale:\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            self.bn(x)\n",
    "        return x\n",
    "    \n",
    "class ResetLinear(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        # print(f\"\\n\\n[ResetLinear] layer: {layer}\\n\\n\")\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm1d(layer.out_features)\n",
    "        self.weight = layer.weight\n",
    "        self.bias = layer.bias\n",
    "\n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: [L, N, C] (in torch.nn.BatchNorm1d doc notation)\n",
    "        x = self.layer(x)\n",
    "\n",
    "        # match current shape to shape required by BatchNorm1d\n",
    "        x = x.permute(1, 2, 0)\n",
    "        # x.shape: [N, C, L] (in torch.nn.BatchNorm1d doc notation)\n",
    "        \n",
    "        x = self.bn(x)\n",
    "\n",
    "        # match current shape to shape required by Linear\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # x.shape: [L, N, C] (in torch.nn.BatchNorm1d doc notation)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class ResetLayerNorm(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm1d(layer.normalized_shape[0])\n",
    "\n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return self.bn(x)\n",
    "\n",
    "\n",
    "def replace_layers(module):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Conv2d):\n",
    "            setattr(module, name, ResetConv(child))\n",
    "        # elif isinstance(child, nn.LayerNorm):\n",
    "        #     setattr(module, name, ResetLayerNorm(child))\n",
    "        elif isinstance(child, nn.Linear):\n",
    "            setattr(module, name, ResetLinear(child))\n",
    "        else:\n",
    "            replace_layers(child)\n",
    "\n",
    "\n",
    "def make_tracked_net(model):\n",
    "    tracked_model = copy.deepcopy(model)\n",
    "    replace_layers(tracked_model.model.visual)\n",
    "    return tracked_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_task_statistics(model, endpoint_models):\n",
    "#     for m_interp, *endpoint_modules in zip(model_to_repair.modules(), *[model.modules() for model in endpoint_models]):\n",
    "#         if isinstance(m_interp, (ResetConv, ResetLayerNorm, ResetLinear)):\n",
    "#             mu_endpoints = torch.stack([m.bn.running_mean for m in endpoint_modules])\n",
    "#             goal_mean = mu_endpoints.mean(dim=0)\n",
    "#             var_endpoints = torch.stack([m.bn.running_var for m in endpoint_modules])\n",
    "#             goal_var = var_endpoints.mean(dim=0)\n",
    "#             m_interp.set_stats(goal_mean, goal_var)\n",
    "#             m_interp.rescale = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap and compute stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\n",
    "    cfg.nn.data.train_dataset,\n",
    "    preprocess_fn=zeroshot_model.train_preprocess,\n",
    "    location=cfg.nn.data.data_path,\n",
    "    batch_size=cfg.nn.data.batch_size.train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_bn_stats(model, epochs, loader):\n",
    "    \"\"\"\n",
    "    Reset batchnorm stats. We use the train loader with data augmentation as this gives better results.\n",
    "    \"\"\"\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None  # use simple average\n",
    "            m.reset_running_stats()\n",
    "\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if isinstance(batch, Dict):\n",
    "                    input = batch[\"x\"]\n",
    "                else:\n",
    "                    input = batch[0]\n",
    "                    # print(f\"[reset_bn_stats] input.shape before model(): {input.shape}\")\n",
    "                out = model(input.cuda())\n",
    "                # print(f\"[reset_bn_stats] out.shape after model()   : {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'CIFAR100'\n",
    "dataset_name = cfg.nn.data.train_dataset.replace('Val', '')\n",
    "finetuned_model = finetuned_models[dataset_name]\n",
    "tracked_finetuned_model = make_tracked_net(finetuned_models[dataset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_bn_stats(tracked_finetuned_model.cuda(), 1, dataset.train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_finetuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda model: parameters_to_vector(model.parameters()) \n",
    "\n",
    "zeroshot_vec = flatten(zeroshot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors = [TaskVector.from_models(zeroshot_model, finetuned_models[dataset]) for dataset in cfg.task_vectors.to_apply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_task_vector(model, task_vector):\n",
    "    model.load_state_dict({k: v + task_vector[k] for k, v in model.state_dict().items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    task_vectors = torch.stack([flatten(finetuned_models[dataset]) - zeroshot_vec for dataset in cfg.task_vectors.to_apply])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors_sum = torch.sum(task_vectors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "\n",
    "multi_task_vector = task_vectors_sum / len(task_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_model = copy.deepcopy(zeroshot_model) \n",
    "vector_to_parameters(multi_task_vector, delta_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_equipped_model = copy.deepcopy(zeroshot_model)\n",
    "apply_task_vector(task_equipped_model, delta_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_identifier = f\"{cfg.nn.module.model.model_name}_{cfg.nn.data.dataset.dataset_name}_head\"\n",
    "classification_head = load_model_from_artifact(\n",
    "    artifact_path=f\"{classification_head_identifier}:latest\", run=logger.experiment\n",
    ")\n",
    "\n",
    "model = hydra.utils.instantiate(\n",
    "    cfg.nn.module, encoder=task_equipped_model, classifier=classification_head, _recursive_=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index_everything(cfg)\n",
    "\n",
    "dataset = get_dataset(\n",
    "    cfg.nn.data.train_dataset,\n",
    "    preprocess_fn=model.encoder.train_preprocess,\n",
    "    location=cfg.nn.data.data_path,\n",
    "    batch_size=cfg.nn.data.batch_size.train,\n",
    ")\n",
    "\n",
    "callbacks: List[Callback] = build_callbacks(cfg.train.callbacks, template_core)\n",
    "\n",
    "storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "pylogger.info(\"Instantiating the <Trainer>\")\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=storage_dir,\n",
    "    plugins=[NNCheckpointIO(jailing_dir=logger.run_dir)],\n",
    "    logger=False,\n",
    "    callbacks=callbacks,\n",
    "    **cfg.train.trainer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylogger.info(\"Evaluating on the training set\")\n",
    "# trainer.test(model=model, dataloaders=dataset.train_loader)\n",
    "\n",
    "pylogger.info(\"Evaluating on the test set!\")\n",
    "trainer.test(model=model, dataloaders=dataset.test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep this accuracy in mind, as it will be used as baseline to compare all upcoming experiments against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dict to store layer-wise stats for each dataset (S in board picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility methods to extract layer-wise stats for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the full path of a module\n",
    "def get_module_path(module, parent_name=\"\"):\n",
    "    if parent_name:\n",
    "        return f\"{parent_name}.{module._get_name()}\"\n",
    "    return module._get_name()\n",
    "\n",
    "\n",
    "# Define a recursive function to collect batch norm stats from 'Reset' layers\n",
    "def collect_bn_stats(model, parent_name=\"\"):\n",
    "    bn_stats = {}\n",
    "    for name, module in model.named_children():\n",
    "        # Construct the full path of the current module\n",
    "        module_path = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        \n",
    "        # Check for batch normalization layers within Reset layers\n",
    "        if isinstance(module, ResetConv) or isinstance(module, ResetLinear):\n",
    "            for sub_name, sub_module in module.named_children():\n",
    "                if isinstance(sub_module, nn.BatchNorm2d) or isinstance(sub_module, nn.BatchNorm1d):\n",
    "                    bn_stats[f\"{module_path}.{sub_name}\"] = {\n",
    "                        \"running_mean\": sub_module.running_mean.clone().detach(),\n",
    "                        \"running_var\": sub_module.running_var.clone().detach(),\n",
    "                        \"num_batches_tracked\": sub_module.num_batches_tracked.clone().detach()\n",
    "                    }\n",
    "        # Recursively check submodules\n",
    "        bn_stats.update(collect_bn_stats(module, module_path))\n",
    "    \n",
    "    return bn_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models\n",
    "\n",
    "(same as at the beginning of the notebook, repeated for hygene reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:35 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_pt:latest                   <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:35\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_pt:latest                   \u001b]8;id=663784;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=459697;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_pt:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:38 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:38\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=355368;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=790453;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:40 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_Cars_0:latest               <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:40\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_Cars_0:latest               \u001b]8;id=527597;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=234146;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_Cars_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:43 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:43\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=836233;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=748715;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:46 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_CIFAR100_0:latest           <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:46\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_CIFAR100_0:latest           \u001b]8;id=650903;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=922805;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_CIFAR100_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:48 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:48\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=351483;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=780251;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:51 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_DTD_0:latest                <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:51\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_DTD_0:latest                \u001b]8;id=219438;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=568033;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_DTD_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:54 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:54\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=889220;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=474666;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:56 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_EuroSAT_0:latest            <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:56\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_EuroSAT_0:latest            \u001b]8;id=829130;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=168546;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_EuroSAT_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:04:59 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:04:59\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=315957;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=494753;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:01 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_GTSRB_0:latest              <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:01\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_GTSRB_0:latest              \u001b]8;id=782604;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=182222;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_GTSRB_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:04 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:04\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=996500;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=369655;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:06 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_MNIST_0:latest              <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:06\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_MNIST_0:latest              \u001b]8;id=270984;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=480906;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_MNIST_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:09 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:09\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=104463;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=575048;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:11 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_RESISC45_0:latest           <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:11\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_RESISC45_0:latest           \u001b]8;id=724270;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=330076;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_RESISC45_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:14 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:14\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=434389;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=806390;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:16 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading model from artifact ViT-B-16_SVHN_0:latest               <a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tvp.utils.io_utils</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:16\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model from artifact ViT-B-16_SVHN_0:latest               \u001b]8;id=229256;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py\u001b\\\u001b[2mtvp.utils.io_utils\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=261810;file:///mnt/KS_2TB/PARA/Projects/task-vectors-playground/src/tvp/utils/io_utils.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViT-B-16_SVHN_0:latest, 426.51MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-B-16 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2024-05-29 20:05:19 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading pretrained ViT-B-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> from OpenAI.                                       <a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">root</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2024-05-29 20:05:19\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading pretrained ViT-B-\u001b[1;36m16\u001b[0m from OpenAI.                                       \u001b]8;id=673489;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py\u001b\\\u001b[2mroot\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=756732;file:///mnt/KS_2TB/PARA/Resources/miniconda3/envs/tvp/lib/python3.11/site-packages/open_clip/factory.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO once done, try without it, as all methods use copy.deepcopy, so hygene should be guaranteed\n",
    "\n",
    "import copy\n",
    "\n",
    "zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "finetuned_models = {\n",
    "    dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "    for dataset in cfg.task_vectors.to_apply\n",
    "}\n",
    "\n",
    "zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate stats for all datasets\n",
    "\n",
    "**TODO** store it as artifact, if experiment is successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gotta populate S for all supported datasets, so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"./dataset_stats.pth\" is on disk, do not compute stats!\n",
    "populate_stats = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset stats from './dataset_stats.pth'...\n"
     ]
    }
   ],
   "source": [
    "if populate_stats:\n",
    "\n",
    "    for dataset_name in cfg.task_vectors.to_apply:\n",
    "\n",
    "        print(f\"[dataset_name]: {dataset_name}\\n\")\n",
    "        \n",
    "        # begin load the dataset\n",
    "\n",
    "        dataset = get_dataset(\n",
    "            cfg.nn.data.train_dataset,\n",
    "            preprocess_fn=zeroshot_model.train_preprocess,\n",
    "            location=cfg.nn.data.data_path,\n",
    "            batch_size=cfg.nn.data.batch_size.train,\n",
    "        )\n",
    "\n",
    "        #   end load the dataset\n",
    "\n",
    "        # begin compute and apply dataset task vector\n",
    "\n",
    "        flatten = lambda model: parameters_to_vector(model.parameters()) \n",
    "\n",
    "        zeroshot_vec = flatten(zeroshot_model)\n",
    "\n",
    "        task_vector = TaskVector.from_models(zeroshot_model, finetuned_models[dataset_name])\n",
    "\n",
    "        task_vector = flatten(finetuned_models[dataset_name]) - zeroshot_vec\n",
    "\n",
    "        # no need to do task_vector_sum-related stuff, as for this step we apply just one task vector\n",
    "\n",
    "        delta_model = copy.deepcopy(zeroshot_model) \n",
    "        vector_to_parameters(multi_task_vector, delta_model.parameters())\n",
    "\n",
    "        task_equipped_model = copy.deepcopy(zeroshot_model)\n",
    "        apply_task_vector(task_equipped_model, delta_model.state_dict())\n",
    "\n",
    "        #   end compute and apply dataset task vector\n",
    "\n",
    "        # begin compute layer-wise statistics\n",
    "\n",
    "        tracked_task_equipped_model = make_tracked_net(task_equipped_model)\n",
    "\n",
    "        reset_bn_stats(tracked_task_equipped_model.cuda(), 1, dataset.train_loader)\n",
    "\n",
    "        #   end compute layer-wise statistics\n",
    "\n",
    "        # begin store layer-wise statistics\n",
    "\n",
    "        dataset_stats[dataset_name] = collect_bn_stats(tracked_task_equipped_model.model.visual)\n",
    "\n",
    "        #   end store layer-wise statistics  \n",
    "\n",
    "    torch.save(dataset_stats, './dataset_stats.pth')\n",
    "\n",
    "else:\n",
    "\n",
    "    print(f\"Loading dataset stats from './dataset_stats.pth'...\")\n",
    "    \n",
    "    dataset_stats = torch.load('./dataset_stats.pth')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute anchors for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_anchor_batches = 1\n",
    "\n",
    "anchors = {}\n",
    "\n",
    "populate_anchors = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anchors from './anchors.pth'...\n"
     ]
    }
   ],
   "source": [
    "if populate_anchors:\n",
    "\n",
    "    for dataset_name in cfg.task_vectors.to_apply:\n",
    "\n",
    "        print(f\"[dataset_name]: {dataset_name}\\n\")\n",
    "\n",
    "        dataset = get_dataset(\n",
    "            cfg.nn.data.train_dataset,\n",
    "            preprocess_fn=zeroshot_model.train_preprocess,\n",
    "            location=cfg.nn.data.data_path,\n",
    "            batch_size=cfg.nn.data.batch_size.train,\n",
    "        )\n",
    "\n",
    "        model = copy.deepcopy(finetuned_models[dataset_name])\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for id, train_batch in enumerate (dataset.train_loader):\n",
    "                    \n",
    "                    if id == n_anchor_batches:                    \n",
    "                        break\n",
    "            \n",
    "                    if isinstance(train_batch, Dict):\n",
    "                        input = train_batch[\"x\"]\n",
    "                    else:\n",
    "                        input = train_batch[0]\n",
    "\n",
    "                    anchors[dataset_name] = model.cuda()(input.cuda())\n",
    "\n",
    "    torch.save(anchors, './anchors.pth')\n",
    "    \n",
    "else:\n",
    "     \n",
    "    print(f\"Loading anchors from './anchors.pth'...\")\n",
    "    \n",
    "    anchors = torch.load('./anchors.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
