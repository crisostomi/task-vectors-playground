{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import hydra\n",
    "import omegaconf\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from lightning.pytorch import Callback\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import enforce_tags, seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "from nn_core.serialization import NNCheckpointIO\n",
    "\n",
    "# Force the execution of __init__.py if this file is executed directly.\n",
    "import tvp  # noqa\n",
    "from tvp.data.datamodule import MetaData\n",
    "from tvp.data.datasets.registry import get_dataset\n",
    "from tvp.task_vectors.task_vectors import TaskVector\n",
    "from tvp.utils.io_utils import load_model_from_artifact\n",
    "from tvp.utils.utils import build_callbacks\n",
    "from torch.nn.utils import vector_to_parameters\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "pylogger = logging.getLogger(__name__)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"playground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"task_vectors\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index_everything(cfg)\n",
    "\n",
    "cfg.core.tags = enforce_tags(cfg.core.get(\"tags\", None))\n",
    "\n",
    "template_core: NNTemplateCore = NNTemplateCore(\n",
    "    restore_cfg=cfg.train.get(\"restore\", None),\n",
    ")\n",
    "logger: NNLogger = NNLogger(logging_cfg=cfg.train.logging, cfg=cfg, resume_id=template_core.resume_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "zeroshot_identifier = f\"{cfg.nn.module.model.model_name}_pt\"\n",
    "\n",
    "zeroshot_model = load_model_from_artifact(artifact_path=f\"{zeroshot_identifier}:latest\", run=logger.experiment)\n",
    "\n",
    "finetuned_id_fn = lambda dataset: f\"{cfg.nn.module.model.model_name}_{dataset}_{cfg.seed_index}:latest\"\n",
    "\n",
    "finetuned_models = {\n",
    "    dataset: load_model_from_artifact(artifact_path=finetuned_id_fn(dataset), run=logger.experiment)\n",
    "    for dataset in cfg.task_vectors.to_apply\n",
    "}\n",
    "\n",
    "zeroshot_orig_weights = copy.deepcopy(zeroshot_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda model: parameters_to_vector(model.parameters())\n",
    "\n",
    "zeroshot_vec = flatten(zeroshot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors = [\n",
    "    TaskVector.from_models(zeroshot_model, finetuned_models[dataset]) for dataset in cfg.task_vectors.to_apply\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_task_vector(model, task_vector):\n",
    "    model.load_state_dict({k: v + task_vector[k] for k, v in model.state_dict().items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    task_vectors = torch.stack(\n",
    "        [flatten(finetuned_models[dataset]) - zeroshot_vec for dataset in cfg.task_vectors.to_apply]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard task vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors_sum = torch.sum(task_vectors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_task_vector = task_vectors_sum / len(task_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_model = copy.deepcopy(zeroshot_model)\n",
    "vector_to_parameters(multi_task_vector, delta_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_equipped_model = copy.deepcopy(zeroshot_model)\n",
    "apply_task_vector(task_equipped_model, delta_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_identifier = f\"{cfg.nn.module.model.model_name}_{cfg.nn.data.dataset.dataset_name}_head\"\n",
    "classification_head = load_model_from_artifact(\n",
    "    artifact_path=f\"{classification_head_identifier}:latest\", run=logger.experiment\n",
    ")\n",
    "\n",
    "model = hydra.utils.instantiate(\n",
    "    cfg.nn.module, encoder=task_equipped_model, classifier=classification_head, _recursive_=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Singular Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute SVDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = cfg.task_vectors.to_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary dataset used as a key\n",
    "ref_dataset = dataset_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get layer task tensors, i.e. deltas for each layer maintaining the tensor structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_task_tensors = {}\n",
    "\n",
    "for dataset, task_vector in zip(dataset_names, task_vectors):\n",
    "    delta_model = copy.deepcopy(zeroshot_model)\n",
    "\n",
    "    vector_to_parameters(task_vector, delta_model.parameters())\n",
    "\n",
    "    layer_task_tensors[dataset] = dict(delta_model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_matrix = lambda x: len(x.shape) == 2\n",
    "layers_to_ignore = {\"model.text_projection\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_results = {}\n",
    "\n",
    "for dataset_name, task_vector in zip(dataset_names, task_vectors):\n",
    "    svd_results[dataset_name] = {}\n",
    "\n",
    "    for layer_name, layer_task_tensor in layer_task_tensors[dataset_name].items():\n",
    "        if is_matrix(layer_task_tensor) and layer_name not in layers_to_ignore:\n",
    "            svd = torch.svd(layer_task_tensor)\n",
    "\n",
    "            svd_results[dataset_name][layer_name] = {\"U\": svd.U, \"S\": svd.S, \"V\": svd.V}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_task_components(x, dim, perc_comps_for_task):\n",
    "    num_comps = int(x.shape[dim] * perc_comps_for_task)\n",
    "\n",
    "    if x.dim() == 1:\n",
    "        return x[:num_comps]\n",
    "\n",
    "    assert x.dim() == 2\n",
    "\n",
    "    if dim == 0:\n",
    "        return x[:num_comps, :]\n",
    "    elif dim == 1:\n",
    "        return x[:, :num_comps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "pow = 2\n",
    "perc_comps_for_task = 1 / len(dataset_names)\n",
    "\n",
    "select_comps = partial(select_task_components, perc_comps_for_task=perc_comps_for_task)\n",
    "\n",
    "task_sing_vectors = {}\n",
    "\n",
    "for layer_name, layer_task_tensor in layer_task_tensors[ref_dataset].items():\n",
    "    if is_matrix(layer_task_tensor) and layer_name not in layers_to_ignore:\n",
    "        U = torch.concat(\n",
    "            [select_comps(svd_results[dataset_name][layer_name][\"U\"], dim=1) for dataset_name in dataset_names], dim=1\n",
    "        ).detach()\n",
    "        S = torch.concat(\n",
    "            [select_comps(svd_results[dataset_name][layer_name][\"S\"], dim=0) for dataset_name in dataset_names]\n",
    "        ).detach()\n",
    "        Vt = torch.concat(\n",
    "            [select_comps(svd_results[dataset_name][layer_name][\"V\"], dim=1).T for dataset_name in dataset_names], dim=0\n",
    "        ).detach()\n",
    "\n",
    "        assert U.shape[1] == S.shape[0] == Vt.shape[0]  # rank\n",
    "        assert U.shape[0] == layer_task_tensor.shape[0] and Vt.shape[1] == layer_task_tensor.shape[1]  # N, M\n",
    "\n",
    "        var_u = torch.pow(\n",
    "            torch.linalg.multi_dot((U.mT, U, torch.diag(S))),\n",
    "            pow,\n",
    "        )\n",
    "        var_v = torch.pow(\n",
    "            torch.linalg.multi_dot((torch.diag(S), Vt, Vt.mT)),\n",
    "            pow,\n",
    "        )\n",
    "\n",
    "        var_u = var_u / (torch.sum(torch.abs(var_u), dim=0) + 1e-12)\n",
    "        var_v = var_v / (torch.sum(torch.abs(var_v), dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "        S_tilde = torch.diagonal(torch.diag(S) @ (var_u * var_v))\n",
    "        assert S_tilde.shape == S.shape\n",
    "\n",
    "        interf = U.mT @ (U @ torch.diag_embed(S) @ Vt) @ Vt.mT\n",
    "\n",
    "        no_interf = (\n",
    "            U.mT\n",
    "            @ torch.linalg.multi_dot(\n",
    "                (\n",
    "                    U,\n",
    "                    torch.diag(S_tilde) @ (var_u * var_v),\n",
    "                    Vt,\n",
    "                )\n",
    "            )\n",
    "            @ Vt.mT\n",
    "        )\n",
    "\n",
    "        task_sing_vec = U @ torch.diag(S_tilde) @ (var_u * var_v) @ Vt\n",
    "\n",
    "        assert task_sing_vec.shape == layer_task_tensor.shape\n",
    "\n",
    "        task_sing_vectors[layer_name] = {\n",
    "            \"u1_u2\": U.mT @ U,\n",
    "            \"s1+s2\": S,\n",
    "            \"tilde_s\": S_tilde,\n",
    "            \"v1_v2\": Vt.mT @ Vt,\n",
    "            \"interf\": interf,\n",
    "            \"no_interf\": no_interf,\n",
    "            \"task_sing_vec\": task_sing_vec,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.ticker as mticker\n",
    "# from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# num_buckets = 11\n",
    "\n",
    "# color_values = np.linspace(-0.5, 0.5, num_buckets + 1)\n",
    "# color_list = []\n",
    "\n",
    "# for i in range(num_buckets):\n",
    "#     color = plt.cm.RdBu((i + 0.5) / num_buckets)  # Adjusting 0.5 to center colors\n",
    "#     color_list.append(color)\n",
    "\n",
    "# # Create a LinearSegmentedColormap with your custom colors\n",
    "# custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", color_list, num_buckets)\n",
    "\n",
    "# for layer_name, layer_task_tensor in delta_model.named_parameters():\n",
    "\n",
    "#     if is_matrix(layer_task_tensor):\n",
    "\n",
    "\n",
    "#         if layer_name not in [\"model.token_embedding.weight\", \"model.positional_embedding\"]:\n",
    "#             print(f\"Plotting  {layer_name}\")\n",
    "\n",
    "#             fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "#             cax = ax1.imshow(\n",
    "#                 result[layer_name][\"u1_u2\"],  # [:25, :25],\n",
    "#                 vmin=-0.5,\n",
    "#                 vmax=0.5,\n",
    "#                 cmap=custom_cmap,\n",
    "#                 aspect=\"auto\",\n",
    "#             )\n",
    "#             ax1.set_title(f\" {layer_name}: sum_u.T @ sum_u\")\n",
    "#             cbar = fig.colorbar(\n",
    "#                 cax,\n",
    "#                 # ticks=[-0.5, 0, 0.5],\n",
    "#                 # format=mticker.FixedFormatter([\"< -0.5\", \"0\", \"> 0.5\"]),\n",
    "#                 ticks=np.linspace(-1, 1, num_buckets),\n",
    "#                 format=mticker.FixedFormatter(\n",
    "#                     [round(x, 2) for x in np.linspace(-1, 1, num_buckets)]\n",
    "#                 ),\n",
    "#                 extend=\"both\",\n",
    "#             )\n",
    "\n",
    "#             cax = ax2.imshow(\n",
    "#                 result[layer_name][\"v1_v2\"],  # [:25, :25],\n",
    "#                 vmin=-0.5,\n",
    "#                 vmax=0.5,\n",
    "#                 cmap=custom_cmap,\n",
    "#                 aspect=\"auto\",\n",
    "#             )\n",
    "#             ax2.set_title(f\"{layer_name}: sum_v @ sum_v.T\")\n",
    "#             cbar = fig.colorbar(\n",
    "#                 cax,\n",
    "#                 # ticks=[-0.5, 0, 0.5],\n",
    "#                 # format=mticker.FixedFormatter([\"< -0.5\", \"0\", \"> 0.5\"]),\n",
    "#                 ticks=np.linspace(-1, 1, num_buckets),\n",
    "#                 format=mticker.FixedFormatter(\n",
    "#                     [round(x, 2) for x in np.linspace(-1, 1, num_buckets)]\n",
    "#                 ),\n",
    "#                 extend=\"both\",\n",
    "#             )\n",
    "#             plt.show()\n",
    "\n",
    "#             fig, axs = plt.subplots(\n",
    "#                 nrows=1, ncols=2, figsize=(20, 10), sharey=False, sharex=False\n",
    "#             )\n",
    "\n",
    "#             axs[0].plot(result[layer_name][\"s1+s2\"])  # axs[0].semilogy(s_anchor1)\n",
    "#             axs[0].set_title(f\"Singular values of the  datasets in concatenation\")\n",
    "#             axs[0].set_xlabel(\"Singular value index\")\n",
    "#             axs[0].set_ylabel(\"Singular value\")\n",
    "\n",
    "#             # the fraction of the energy captured by the first r singular values\n",
    "#             axs[1].plot(\n",
    "#                 np.cumsum(result[layer_name][\"s1+s2\"]) / torch.sum(result[layer_name][\"s1+s2\"])\n",
    "#             )\n",
    "#             axs[1].set_title(\"Cumulative sum of the singular values\")\n",
    "#             axs[1].set_xlabel(\"Singular value index\")\n",
    "#             axs[1].set_ylabel(\"Cumulative sum\")\n",
    "#             plt.show()\n",
    "\n",
    "#             fig, axs = plt.subplots(\n",
    "#                 nrows=1, ncols=2, figsize=(20, 10), sharey=False, sharex=False\n",
    "#             )\n",
    "\n",
    "#             axs[0].plot(result[layer_name][\"tilde_s\"])  # axs[0].semilogy(s_anchor1)\n",
    "#             axs[0].set_title(f\"Singular values of the  datasets in new Sigma\")\n",
    "#             axs[0].set_xlabel(\"Singular value index\")\n",
    "#             axs[0].set_ylabel(\"Singular value\")\n",
    "\n",
    "#             # the fraction of the energy captured by the first r singular values\n",
    "#             axs[1].plot(\n",
    "#                 np.cumsum(result[layer_name][\"tilde_s\"]) / torch.sum(result[layer_name][\"tilde_s\"])\n",
    "#             )\n",
    "#             axs[1].set_title(\"Cumulative sum of the singular values\")\n",
    "#             axs[1].set_xlabel(\"Singular value index\")\n",
    "#             axs[1].set_ylabel(\"Cumulative sum\")\n",
    "#             plt.show()\n",
    "\n",
    "#             fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "#             cax = ax1.imshow(\n",
    "#                 result[layer_name][\"interf\"],  # [:25, :25],\n",
    "#                 vmin=-0.01,\n",
    "#                 vmax=0.01,\n",
    "#                 cmap=custom_cmap,\n",
    "#                 aspect=\"auto\",\n",
    "#             )\n",
    "#             ax1.set_title(f\" {layer_name}: interference\")\n",
    "#             cbar = fig.colorbar(\n",
    "#                 cax,\n",
    "#                 # ticks=[-0.5, 0, 0.5],\n",
    "#                 # format=mticker.FixedFormatter([\"< -0.5\", \"0\", \"> 0.5\"]),\n",
    "#                 ticks=np.linspace(-0.01, 0.01, num_buckets),\n",
    "#                 format=mticker.FixedFormatter(\n",
    "#                     [round(x, 2) for x in np.linspace(-0.01, 0.01, num_buckets)]\n",
    "#                 ),\n",
    "#                 extend=\"both\",\n",
    "#             )\n",
    "\n",
    "#             cax = ax2.imshow(\n",
    "#                 result[layer_name][\"no_interf\"],  # [:25, :25],\n",
    "#                 vmin=-0.005,\n",
    "#                 vmax=0.005,\n",
    "#                 cmap=custom_cmap,\n",
    "#                 aspect=\"auto\",\n",
    "#             )\n",
    "#             ax2.set_title(f\"{layer_name}: reduced interference\")\n",
    "#             cbar = fig.colorbar(\n",
    "#                 cax,\n",
    "#                 # ticks=[-0.5, 0, 0.5],\n",
    "#                 # format=mticker.FixedFormatter([\"< -0.5\", \"0\", \"> 0.5\"]),\n",
    "#                 ticks=np.linspace(-0.005, 0.005, num_buckets),\n",
    "#                 format=mticker.FixedFormatter(\n",
    "#                     [round(x, 3) for x in np.linspace(-0.005, 0.005, num_buckets)]\n",
    "#                 ),\n",
    "#                 extend=\"both\",\n",
    "#             )\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SVD multi-task vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(layer_task_tensors[ref_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_layer_task_tensors = {}\n",
    "\n",
    "for layer_name, layer_tensor in layer_task_tensors[ref_dataset].items():\n",
    "    if is_matrix(layer_tensor) and layer_name not in layers_to_ignore:\n",
    "        merged_layer_task_tensors[layer_name] = task_sing_vectors[layer_name][\"task_sing_vec\"]\n",
    "\n",
    "    else:\n",
    "        merged_layer_task_tensors[layer_name] = sum(\n",
    "            [layer_task_tensors[dataset_name][layer_name] for dataset_name in dataset_names]\n",
    "        ) / len(dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply multi-task vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_equipped_model = copy.deepcopy(zeroshot_model)\n",
    "\n",
    "apply_task_vector(task_equipped_model, merged_layer_task_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head_identifier = f\"{cfg.nn.module.model.model_name}_{cfg.nn.data.dataset.dataset_name}_head\"\n",
    "classification_head = load_model_from_artifact(\n",
    "    artifact_path=f\"{classification_head_identifier}:latest\", run=logger.experiment\n",
    ")\n",
    "\n",
    "model = hydra.utils.instantiate(\n",
    "    cfg.nn.module, encoder=task_equipped_model, classifier=classification_head, _recursive_=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index_everything(cfg)\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "for dataset in cfg.eval_datasets:\n",
    "    classification_head_identifier = f\"{cfg.nn.module.model.model_name}_{dataset}_head\"\n",
    "    classification_head = load_model_from_artifact(\n",
    "        artifact_path=f\"{classification_head_identifier}:latest\", run=logger.experiment\n",
    "    )\n",
    "\n",
    "    model = hydra.utils.instantiate(\n",
    "        cfg.nn.module, encoder=task_equipped_model, classifier=classification_head, _recursive_=False\n",
    "    )\n",
    "\n",
    "    dataset = get_dataset(\n",
    "        dataset,\n",
    "        preprocess_fn=model.encoder.train_preprocess,\n",
    "        location=cfg.nn.data.data_path,\n",
    "        batch_size=cfg.nn.data.batch_size.train,\n",
    "    )\n",
    "\n",
    "    callbacks: List[Callback] = build_callbacks(cfg.train.callbacks, template_core)\n",
    "\n",
    "    storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "    pylogger.info(\"Instantiating the <Trainer>\")\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=storage_dir,\n",
    "        plugins=[NNCheckpointIO(jailing_dir=logger.run_dir)],\n",
    "        logger=False,\n",
    "        callbacks=callbacks,\n",
    "        **cfg.train.trainer,\n",
    "    )\n",
    "\n",
    "    pylogger.info(\"Evaluating on the test set!\")\n",
    "    dataset_results = trainer.test(model=model, dataloaders=dataset.test_loader)\n",
    "\n",
    "    accuracies[dataset] = dataset_results[0][\"acc/test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = sum(accuracies.values()) / len(accuracies)\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pylogger.info(\"Evaluating on the training set\")\n",
    "# # trainer.test(model=model, dataloaders=dataset.train_loader)\n",
    "\n",
    "# pylogger.info(\"Evaluating on the test set!\")\n",
    "# trainer.test(model=model, dataloaders=dataset.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
